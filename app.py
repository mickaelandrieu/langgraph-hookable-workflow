# Installation requise:
# pip install langgraph langchain-core scikit-learn numpy sentence-transformers

from typing import TypedDict, List, Dict, Any, Optional, Callable, Annotated
from abc import ABC, abstractmethod
import asyncio
import time
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

# LangGraph imports
from langgraph.graph import StateGraph
from langchain_core.runnables import RunnableConfig
from operator import add

# Fonction de merge pour les dictionnaires
def merge_dicts(a: dict, b: dict) -> dict:
    """Merge deux dictionnaires de mani√®re s√ªre"""
    result = a.copy() if a else {}
    if b:
        result.update(b)
    return result


# ============================================================================
# √âTAT DU GRAPHE AVEC ANNOTATIONS
# ============================================================================

class SearchState(TypedDict):
    """√âtat partag√© du graphe LangGraph avec annotations pour la concurrence"""
    # Donn√©es de requ√™te (pas de conflit)
    original_query: str
    processed_query: str
    query_expansion: List[str]
    query_intent: str
    
    # R√©sultats de recherche - ANNOTATED pour √©viter les conflits concurrents
    vector_results: Annotated[List[Dict[str, Any]], add]
    keyword_results: Annotated[List[Dict[str, Any]], add]
    combined_results: List[Dict[str, Any]]
    final_results: List[Dict[str, Any]]
    
    # M√©tadonn√©es - ANNOTATED avec fonction de merge personnalis√©e
    processing_metadata: Annotated[Dict[str, Any], merge_dicts]
    performance_metrics: Annotated[Dict[str, float], merge_dicts]
    
    # IMPORTANT: R√©ponse LLM finale
    llm_response: str


# ============================================================================
# INTERFACES POUR LES HOOKS
# ============================================================================

class PreprocessorHook(ABC):
    """Interface pour les hooks de pr√©processing"""
    
    @abstractmethod
    async def clean_query(self, query: str) -> str:
        """Nettoyage de la requ√™te"""
        pass
    
    @abstractmethod
    async def expand_query(self, query: str) -> List[str]:
        """Expansion de la requ√™te"""
        pass
    
    @abstractmethod
    async def detect_intent(self, query: str) -> str:
        """D√©tection d'intention"""
        pass


class SearcherHook(ABC):
    """Interface pour les hooks de recherche"""
    
    @abstractmethod
    async def search(self, queries: List[str], method: str) -> List[Dict[str, Any]]:
        """Recherche avec m√©thode sp√©cifi√©e"""
        pass


class PostprocessorHook(ABC):
    """Interface pour les hooks de post-processing"""
    
    @abstractmethod
    async def combine_results(self, vector_results: List[Dict], 
                            keyword_results: List[Dict]) -> List[Dict]:
        """Combinaison des r√©sultats"""
        pass
    
    @abstractmethod
    async def rerank_results(self, results: List[Dict], 
                           query: str, intent: str) -> List[Dict]:
        """Re-ranking des r√©sultats"""
        pass


class LLMHook(ABC):
    """Interface pour les hooks LLM"""
    
    @abstractmethod
    async def generate_response(self, query: str, context: str, intent: str) -> str:
        """G√©n√©ration de r√©ponse"""
        pass


# ============================================================================
# IMPL√âMENTATIONS PAR D√âFAUT
# ============================================================================

class DefaultPreprocessor(PreprocessorHook):
    """Impl√©mentation par d√©faut du pr√©processing"""
    
    async def clean_query(self, query: str) -> str:
        """Nettoyage basique"""
        import re
        cleaned = re.sub(r'[^\w\s\-\.√©√®√†√π√ß]', '', query, flags=re.IGNORECASE)
        cleaned = re.sub(r'\s+', ' ', cleaned).strip().lower()
        return cleaned
    
    async def expand_query(self, query: str) -> List[str]:
        """Expansion basique"""
        expansions = [query]
        if "ml" in query:
            expansions.append(query.replace("ml", "machine learning"))
        if "python" in query:
            expansions.append(query + " programmation")
        return expansions
    
    async def detect_intent(self, query: str) -> str:
        """D√©tection d'intention basique"""
        if any(word in query for word in ["qu'est", "d√©finir"]):
            return "definition"
        return "general"


class DefaultSearcher(SearcherHook):
    """Impl√©mentation par d√©faut de la recherche"""
    
    def __init__(self):
        # Chargement des mod√®les d'embeddings
        try:
            from sentence_transformers import SentenceTransformer
            self.embedding_model = SentenceTransformer('all-MiniLM-L6-v2')
            print("‚úÖ Sentence-transformers charg√©")
        except ImportError:
            self.embedding_model = self._mock_embedding_model()
            print("‚ö†Ô∏è Utilisation d'embeddings simul√©s")
        
        # TF-IDF
        self.tfidf = TfidfVectorizer(
            stop_words='english',
            ngram_range=(1, 2),
            max_features=1000
        )
        
        # Documents de test
        self.documents = [
            {"id": 1, "content": "Python est un langage de programmation polyvalent utilis√© en data science", 
             "title": "Introduction Python", "category": "programming"},
            {"id": 2, "content": "Machine learning permet aux ordinateurs d'apprendre sans programmation explicite", 
             "title": "ML Basics", "category": "ai"},
            {"id": 3, "content": "Les r√©seaux de neurones artificiels s'inspirent du cerveau humain", 
             "title": "Neural Networks", "category": "ai"},
            {"id": 4, "content": "LangGraph facilite la cr√©ation de workflows complexes avec des LLMs", 
             "title": "LangGraph Guide", "category": "tools"},
            {"id": 5, "content": "La recherche vectorielle utilise des embeddings pour la similarit√©", 
             "title": "Vector Search", "category": "search"}
        ]
        
        # Pr√©paration des donn√©es
        self.contents = [doc['content'] for doc in self.documents]
        self.doc_embeddings = self.embedding_model.encode(self.contents)
        self.tfidf_matrix = self.tfidf.fit_transform(self.contents)
    
    def _mock_embedding_model(self):
        """Mod√®le d'embedding simul√©"""
        class MockModel:
            def encode(self, texts):
                if isinstance(texts, str):
                    texts = [texts]
                embeddings = []
                for text in texts:
                    vector = np.array([hash(text + str(i)) % 1000 / 1000.0 for i in range(384)])
                    embeddings.append(vector)
                return np.array(embeddings)
        return MockModel()
    
    async def search(self, queries: List[str], method: str) -> List[Dict[str, Any]]:
        """Recherche selon la m√©thode sp√©cifi√©e"""
        if method == "vector":
            return await self._vector_search(queries)
        elif method == "keyword":
            return await self._keyword_search(queries)
        else:
            raise ValueError("M√©thode de recherche inconnue: " + method)
    
    async def _vector_search(self, queries: List[str]) -> List[Dict[str, Any]]:
        """Recherche vectorielle"""
        all_results = {}
        for i, query in enumerate(queries):
            query_embedding = self.embedding_model.encode([query])
            similarities = cosine_similarity(query_embedding, self.doc_embeddings)[0]
            
            weight = 1.0 / (i + 1)
            for idx, similarity in enumerate(similarities):
                doc_id = self.documents[idx]["id"]
                if doc_id not in all_results:
                    all_results[doc_id] = {
                        "document": self.documents[idx],
                        "score": 0,
                        "method": "vector"
                    }
                all_results[doc_id]["score"] += similarity * weight
        
        results = list(all_results.values())
        if results:
            max_score = max(r["score"] for r in results)
            for r in results:
                r["score"] = r["score"] / max_score if max_score > 0 else 0
        
        return sorted(results, key=lambda x: x["score"], reverse=True)[:5]
    
    async def _keyword_search(self, queries: List[str]) -> List[Dict[str, Any]]:
        """Recherche par mots-cl√©s"""
        all_results = {}
        for i, query in enumerate(queries):
            try:
                query_tfidf = self.tfidf.transform([query])
                similarities = cosine_similarity(query_tfidf, self.tfidf_matrix)[0]
                
                weight = 1.0 / (i + 1)
                for idx, similarity in enumerate(similarities):
                    if similarity > 0:
                        doc_id = self.documents[idx]["id"]
                        if doc_id not in all_results:
                            all_results[doc_id] = {
                                "document": self.documents[idx],
                                "score": 0,
                                "method": "keyword"
                            }
                        all_results[doc_id]["score"] += similarity * weight
            except Exception as e:
                print("   ‚ö†Ô∏è Erreur TF-IDF pour '" + query + "': " + str(e))
                continue
        
        results = list(all_results.values())
        if results:
            max_score = max(r["score"] for r in results)
            for r in results:
                r["score"] = r["score"] / max_score if max_score > 0 else 0
        
        return sorted(results, key=lambda x: x["score"], reverse=True)[:5]


class DefaultPostprocessor(PostprocessorHook):
    """Impl√©mentation par d√©faut du post-processing"""
    
    async def combine_results(self, vector_results: List[Dict], 
                            keyword_results: List[Dict]) -> List[Dict]:
        """Combinaison standard"""
        alpha = 0.6
        combined_scores = {}
        
        for result in vector_results:
            doc_id = result["document"]["id"]
            combined_scores[doc_id] = {
                "document": result["document"],
                "vector_score": result["score"],
                "keyword_score": 0.0,
                "combined_score": alpha * result["score"]
            }
        
        for result in keyword_results:
            doc_id = result["document"]["id"]
            if doc_id in combined_scores:
                combined_scores[doc_id]["keyword_score"] = result["score"]
                combined_scores[doc_id]["combined_score"] += (1 - alpha) * result["score"]
            else:
                combined_scores[doc_id] = {
                    "document": result["document"],
                    "vector_score": 0.0,
                    "keyword_score": result["score"],
                    "combined_score": (1 - alpha) * result["score"]
                }
        
        return list(combined_scores.values())
    
    async def rerank_results(self, results: List[Dict], 
                           query: str, intent: str) -> List[Dict]:
        """Re-ranking basique"""
        for result in results:
            boost = 1.0
            if intent == "definition":
                if any(word in result["document"]["content"].lower() 
                       for word in ["d√©finition", "est", "permet"]):
                    boost = 1.2
            
            result["combined_score"] *= boost
            result["intent_boost"] = boost
        
        return sorted(results, key=lambda x: x["combined_score"], reverse=True)


class DefaultLLM(LLMHook):
    """Impl√©mentation par d√©faut du LLM"""
    
    async def generate_response(self, query: str, context: str, intent: str) -> str:
        """G√©n√©ration avec OpenAI ou fallback"""
        try:
            # Tentative d'utilisation d'OpenAI
            try:
                from langchain_openai import ChatOpenAI
                from langchain_core.messages import HumanMessage, SystemMessage
                
                llm = ChatOpenAI(
                    model="gpt-4o-mini",
                    temperature=0.7,
                    max_tokens=500
                )
                
                system_prompt = """Tu es un assistant expert en recherche d'information. 
Tu dois fournir une r√©ponse pr√©cise et concise bas√©e uniquement sur les documents fournis.
Structure ta r√©ponse de mani√®re claire et cite les sources pertinentes."""
                
                user_prompt = "Question de l'utilisateur: " + query + "\n"
                user_prompt += "Intention d√©tect√©e: " + intent + "\n\n"
                user_prompt += "Documents trouv√©s par la recherche hybride:\n" + context + "\n\n"
                user_prompt += "Fournis une r√©ponse compl√®te et structur√©e en te basant sur ces documents."
                
                messages = [
                    SystemMessage(content=system_prompt),
                    HumanMessage(content=user_prompt)
                ]
                
                response = llm.invoke(messages)
                return response.content
                
            except ImportError:
                # Fallback si OpenAI non disponible
                return self._fallback_response(query, context, intent)
                
        except Exception as e:
            print("   ‚ùå Erreur LLM: " + str(e))
            return self._error_response(query, context, str(e))
    
    def _fallback_response(self, query: str, context: str, intent: str) -> str:
        """R√©ponse de fallback"""
        response_title = "**R√©ponse simul√©e** (OpenAI non configur√©)\n\n"
        question_part = "Pour votre question: \"" + query + "\"\n\n"
        context_part = "Bas√© sur la recherche hybride, voici les documents les plus pertinents:\n\n" + context + "\n\n"
        summary_part = "**R√©sum√©**: D'apr√®s ces sources, " + query + " fait r√©f√©rence √† des concepts importants en technologie et intelligence artificielle.\n\n"
        note_part = "*Note: Pour obtenir une r√©ponse g√©n√©r√©e par IA, configurez votre cl√© API OpenAI*"
        
        return response_title + question_part + context_part + summary_part + note_part
    
    def _error_response(self, query: str, context: str, error: str) -> str:
        """R√©ponse en cas d'erreur"""
        error_intro = "**Erreur lors de l'appel LLM**: " + error + "\n\n"
        fallback_intro = "**R√©ponse de secours bas√©e sur la recherche:**\n\n"
        config_note = "\n\nVeuillez v√©rifier votre configuration OpenAI."
        
        return error_intro + fallback_intro + context + config_note


# ============================================================================
# GESTIONNAIRE DE HOOKS
# ============================================================================

class HookManager:
    """Gestionnaire centralis√© des hooks"""
    
    def __init__(self):
        # Hooks par d√©faut
        self.preprocessor = DefaultPreprocessor()
        self.searcher = DefaultSearcher()
        self.postprocessor = DefaultPostprocessor()
        self.llm = DefaultLLM()
        
        print("‚úÖ Gestionnaire de hooks initialis√© avec impl√©mentations par d√©faut")
    
    def register_preprocessor(self, hook: PreprocessorHook, name: str = "custom"):
        """Enregistrer un hook de pr√©processing"""
        self.preprocessor = hook
        print("üîß Hook preprocessor '" + name + "' enregistr√©")
    
    def register_searcher(self, hook: SearcherHook, name: str = "custom"):
        """Enregistrer un hook de recherche"""
        self.searcher = hook
        print("üîß Hook searcher '" + name + "' enregistr√©")
    
    def register_postprocessor(self, hook: PostprocessorHook, name: str = "custom"):
        """Enregistrer un hook de post-processing"""
        self.postprocessor = hook
        print("üîß Hook postprocessor '" + name + "' enregistr√©")
    
    def register_llm(self, hook: LLMHook, name: str = "custom"):
        """Enregistrer un hook LLM"""
        self.llm = hook
        print("üîß Hook LLM '" + name + "' enregistr√©")


# ============================================================================
# N≈íUDS DU GRAPHE UTILISANT LES HOOKS
# ============================================================================

def preprocess_node(state: SearchState, config: RunnableConfig) -> SearchState:
    """N≈íUD 1: Pr√©-traitement avec hooks"""
    print("üîß [NODE] Pr√©-traitement: '" + state['original_query'] + "'")
    
    start_time = time.time()
    hook_manager = config.get("configurable", {}).get("hook_manager")
    
    async def process():
        # Utilisation des hooks
        cleaned = await hook_manager.preprocessor.clean_query(state["original_query"])
        expansions = await hook_manager.preprocessor.expand_query(cleaned)
        intent = await hook_manager.preprocessor.detect_intent(state["original_query"])
        
        print("   ‚úì Expansions: " + str(expansions))
        print("   ‚úì Intention: " + intent)
        
        return {
            "processed_query": cleaned,
            "query_expansion": expansions,
            "query_intent": intent,
            "processing_metadata": {"preprocess_time": time.time() - start_time}
        }
    
    return asyncio.run(process())


def vector_search_node(state: SearchState, config: RunnableConfig) -> SearchState:
    """N≈íUD 2: Recherche vectorielle avec hooks"""
    print("üîç [NODE] Recherche vectorielle...")
    
    start_time = time.time()
    hook_manager = config.get("configurable", {}).get("hook_manager")
    
    async def search():
        results = await hook_manager.searcher.search(state["query_expansion"], "vector")
        print("   ‚úì " + str(len(results)) + " r√©sultats vectoriels")
        
        return {
            "vector_results": results,
            "performance_metrics": {"vector_search_time": time.time() - start_time}
        }
    
    return asyncio.run(search())


def keyword_search_node(state: SearchState, config: RunnableConfig) -> SearchState:
    """N≈íUD 3: Recherche par mots-cl√©s avec hooks"""
    print("üìù [NODE] Recherche par mots-cl√©s...")
    
    start_time = time.time()
    hook_manager = config.get("configurable", {}).get("hook_manager")
    
    async def search():
        results = await hook_manager.searcher.search(state["query_expansion"], "keyword")
        print("   ‚úì " + str(len(results)) + " r√©sultats par mots-cl√©s")
        
        return {
            "keyword_results": results,
            "performance_metrics": {"keyword_search_time": time.time() - start_time}
        }
    
    return asyncio.run(search())


def combine_results_node(state: SearchState, config: RunnableConfig) -> SearchState:
    """N≈íUD 4: Combinaison avec hooks"""
    print("üîÑ [NODE] Combinaison des r√©sultats...")
    
    start_time = time.time()
    hook_manager = config.get("configurable", {}).get("hook_manager")
    
    async def combine():
        # Combinaison avec hook
        combined = await hook_manager.postprocessor.combine_results(
            state["vector_results"], 
            state["keyword_results"]
        )
        
        # Re-ranking avec hook
        reranked = await hook_manager.postprocessor.rerank_results(
            combined, state["processed_query"], state["query_intent"]
        )
        
        print("   ‚úì " + str(len(reranked)) + " r√©sultats combin√©s")
        
        return {
            "combined_results": reranked,
            "final_results": reranked[:3],
            "performance_metrics": {"combine_time": time.time() - start_time}
        }
    
    return asyncio.run(combine())


def llm_response_node(state: SearchState, config: RunnableConfig) -> SearchState:
    """N≈íUD 5: G√©n√©ration LLM avec hooks"""
    print("ü§ñ [NODE] G√©n√©ration r√©ponse LLM...")
    
    start_time = time.time()
    hook_manager = config.get("configurable", {}).get("hook_manager")
    
    async def generate():
        # Construction du contexte
        context_docs = []
        for result in state["final_results"]:
            doc = result["document"]
            score = result.get("combined_score", 0)
            score_text = "{:.3f}".format(score)
            context_docs.append("- **" + doc['title'] + "** (score: " + score_text + "): " + doc['content'])
        
        context = "\n".join(context_docs)
        
        # G√©n√©ration avec hook
        response = await hook_manager.llm.generate_response(
            state["original_query"], context, state["query_intent"]
        )
        
        print("   ‚úÖ R√©ponse LLM g√©n√©r√©e")
        
        return {
            "llm_response": response,
            "performance_metrics": {"llm_time": time.time() - start_time}
        }
    
    return asyncio.run(generate())


# ============================================================================
# CR√âATION DU GRAPHE AVEC HOOKS
# ============================================================================

def create_hookable_hybrid_search_graph(hook_manager: HookManager) -> StateGraph:
    """Cr√©ation du graphe avec syst√®me de hooks"""
    
    graph = StateGraph(SearchState)
    
    # Ajout des n≈ìuds utilisant les hooks
    graph.add_node("preprocess", preprocess_node)
    graph.add_node("vector_search", vector_search_node)
    graph.add_node("keyword_search", keyword_search_node)
    graph.add_node("combine", combine_results_node)
    graph.add_node("llm_response", llm_response_node)
    
    # D√©finition du flux
    graph.set_entry_point("preprocess")
    graph.add_edge("preprocess", "vector_search")
    graph.add_edge("preprocess", "keyword_search")
    graph.add_edge(["vector_search", "keyword_search"], "combine")
    graph.add_edge("combine", "llm_response")
    graph.set_finish_point("llm_response")
    
    return graph.compile()


# ============================================================================
# FONCTION PRINCIPALE AVEC HOOKS
# ============================================================================

async def run_hookable_hybrid_search(query: str, hook_manager: HookManager = None) -> SearchState:
    """Fonction principale avec syst√®me de hooks"""
    if hook_manager is None:
        hook_manager = HookManager()
    
    print("\nüöÄ D√âMARRAGE RECHERCHE HYBRIDE AVEC HOOKS")
    print("=" * 60)
    print("Requ√™te: '" + query + "'")
    print("=" * 60)
    
    # Cr√©ation du graphe avec hooks
    app = create_hookable_hybrid_search_graph(hook_manager)
    
    # √âtat initial
    initial_state = SearchState(
        original_query=query,
        processed_query="",
        query_expansion=[],
        query_intent="",
        vector_results=[],
        keyword_results=[],
        combined_results=[],
        final_results=[],
        processing_metadata={},
        performance_metrics={},
        llm_response=""
    )
    
    # Configuration avec hook manager
    config = RunnableConfig(
        configurable={"hook_manager": hook_manager}
    )
    
    try:
        result = await app.ainvoke(initial_state, config)
        
        print("\n" + "=" * 60)
        print("üìã R√âSULTATS FINAUX:")
        print("=" * 60)
        
        print("\nüéØ Top 3 documents:")
        for i, res in enumerate(result["final_results"], 1):
            doc = res["document"]
            score_text = "{:.3f}".format(res["combined_score"])
            print("   " + str(i) + ". " + doc['title'] + " (score: " + score_text + ")")
            print("      " + doc['content'][:80] + "...")
        
        print("\nü§ñ R√âPONSE LLM:")
        print(result["llm_response"])
        
        print("\n‚è±Ô∏è M√©triques de performance:")
        for metric, value in result["performance_metrics"].items():
            if isinstance(value, (int, float)):
                time_text = "{:.3f}s".format(value)
                print("   " + metric + ": " + time_text)
            else:
                print("   " + metric + ": " + str(value))
        
        return result
        
    except Exception as e:
        print("‚ùå Erreur: " + str(e))
        raise


# ============================================================================
# EXEMPLE D'UTILISATION AVEC HOOKS PERSONNALIS√âS
# ============================================================================

# Hook personnalis√© pour d√©monstration
class CustomPreprocessor(PreprocessorHook):
    """Pr√©processeur personnalis√© avec validation avanc√©e"""
    
    async def clean_query(self, query: str) -> str:
        # Validation de longueur
        if len(query) < 3:
            return query  # Pas de nettoyage pour les requ√™tes courtes
        
        # Nettoyage avanc√©
        import re
        query = re.sub(r'[^\w\s\-\?]', '', query.lower().strip())
        return query
    
    async def expand_query(self, query: str) -> List[str]:
        # Expansion avec synonymes √©tendus
        expansions = [query]
        
        synonyms = {
            "python": ["python", "programming", "d√©veloppement", "langage"],
            "ml": ["machine learning", "apprentissage automatique", "intelligence artificielle"],
            "ai": ["intelligence artificielle", "artificial intelligence", "IA"]
        }
        
        for term, syns in synonyms.items():
            if term in query.lower():
                for syn in syns:
                    exp = query.replace(term, syn)
                    if exp not in expansions:
                        expansions.append(exp)
        
        return expansions[:5]  # Limiter √† 5
    
    async def detect_intent(self, query: str) -> str:
        # D√©tection d'intention plus sophistiqu√©e
        patterns = {
            "definition": ["qu'est-ce", "d√©finir", "d√©finition", "c'est quoi"],
            "how_to": ["comment", "√©tapes", "proc√©dure", "tutoriel"],
            "comparison": ["diff√©rence", "versus", "vs", "comparer"],
            "example": ["exemple", "cas d'usage", "illustration"]
        }
        
        query_lower = query.lower()
        for intent, words in patterns.items():
            if any(word in query_lower for word in words):
                return intent
        
        return "general"


async def demo_with_custom_hooks():
    """D√©monstration avec hooks personnalis√©s"""
    
    print("üîß D√âMONSTRATION AVEC HOOKS PERSONNALIS√âS")
    print("=" * 60)
    
    # Cr√©ation du gestionnaire de hooks
    hook_manager = HookManager()
    
    # Enregistrement d'un hook personnalis√©
    hook_manager.register_preprocessor(CustomPreprocessor(), "advanced")
    
    # Test avec hook personnalis√©
    result = await run_hookable_hybrid_search(
        "qu'est-ce que Python pour le ML ?", 
        hook_manager
    )
    
    print("\nüéØ M√©tadonn√©es de traitement:")
    for key, value in result["processing_metadata"].items():
        print("   " + key + ": " + str(value))


# ============================================================================
# FONCTION PRINCIPALE
# ============================================================================

async def main():
    """Fonction principale de test"""
    
    # Test 1: Avec hooks par d√©faut
    print("=" * 80)
    print("TEST 1: HOOKS PAR D√âFAUT")
    print("=" * 80)
    
    result1 = await run_hookable_hybrid_search("qu'est-ce que Python ?")
    
    print("\n" + "=" * 80)
    input("Appuyez sur Entr√©e pour tester les hooks personnalis√©s...")
    
    # Test 2: Avec hooks personnalis√©s
    print("\n" + "=" * 80)
    print("TEST 2: HOOKS PERSONNALIS√âS")
    print("=" * 80)
    
    await demo_with_custom_hooks()


if __name__ == "__main__":
    print("üîç SYST√àME HYBRID SEARCH AVEC ARCHITECTURE DE HOOKS")
    print("Ce syst√®me permet de remplacer chaque √©tape par une impl√©mentation personnalis√©e")
    
    asyncio.run(main())

# ğŸ” Hybrid Search avec LangGraph et Architecture de Hooks

SystÃ¨me de recherche hybride modulaire combinant recherche vectorielle et par mots-clÃ©s, orchestrÃ© par LangGraph avec systÃ¨me de hooks complet pour remplacer chaque implÃ©mentation.

## ğŸ¯ Vue d'ensemble

Ce projet implÃ©mente un workflow de recherche hybride avec **5 nÅ“uds LangGraph** et un **systÃ¨me de hooks** qui permet de :
- Combiner recherche vectorielle (embeddings) et recherche par mots-clÃ©s (TF-IDF)
- Remplacer chaque Ã©tape par une implÃ©mentation personnalisÃ©e via des hooks
- Utiliser LangGraph pour orchestrer le pipeline
- GÃ©nÃ©rer une rÃ©ponse finale avec OpenAI GPT-4o-mini

## ğŸ—ï¸ Architecture du Graphe LangGraph

### NÅ“uds du Graphe

| NÅ“ud | Fonction | Ordre | Hook Interface | Description |
|------|----------|-------|----------------|-------------|
| **1. preprocess** | `preprocess_node()` | ğŸ”´ **Critique** | `PreprocessorHook` | Nettoyage, expansion, dÃ©tection d'intention |
| **2. vector_search** | `vector_search_node()` | ğŸŸ¢ **ParallÃ¨le** | `SearcherHook` | Recherche avec embeddings (similaritÃ© cosinus) |
| **3. keyword_search** | `keyword_search_node()` | ğŸŸ¢ **ParallÃ¨le** | `SearcherHook` | Recherche TF-IDF par mots-clÃ©s |
| **4. combine** | `combine_results_node()` | ğŸ”´ **Critique** | `PostprocessorHook` | Fusion scores + re-ranking |
| **5. llm_response** | `llm_response_node()` | ğŸ”´ **Critique** | `LLMHook` | GÃ©nÃ©ration rÃ©ponse OpenAI |

### Flux d'ExÃ©cution

```
preprocess â†’ vector_search   â†’ combine â†’ llm_response
          â†˜ keyword_search â†—
```

**Recherches en parallÃ¨le** : `vector_search` et `keyword_search` s'exÃ©cutent simultanÃ©ment aprÃ¨s `preprocess`.

### Workflow DÃ©taillÃ© avec Hooks

```mermaid
graph TD
    A[ğŸš€ RequÃªte Utilisateur] --> B[ğŸ“¥ Ã‰tat Initial LangGraph]
    B --> C[ğŸ”§ NÅ’UD 1: preprocess_node]
    
    C --> D[ğŸª PreprocessorHook.clean_query]
    D --> E[ğŸª PreprocessorHook.detect_intent]
    E --> F[ğŸª PreprocessorHook.expand_query]
    F --> G[ğŸ“¤ Ã‰tat: query + expansions + intent]
    
    G --> H[ğŸ” NÅ’UD 2: vector_search_node]
    G --> I[ğŸ“ NÅ’UD 3: keyword_search_node]
    
    H --> J[ğŸª SearcherHook.search<br/>method='vector']
    I --> K[ğŸª SearcherHook.search<br/>method='keyword']
    
    J --> L[ğŸ“¤ Ã‰tat: vector_results]
    K --> M[ğŸ“¤ Ã‰tat: keyword_results]
    
    L --> N[ğŸ”„ NÅ’UD 4: combine_results_node]
    M --> N
    
    N --> O[ğŸª PostprocessorHook.combine_results]
    O --> P[ğŸª PostprocessorHook.rerank_results]
    P --> Q[ğŸ“¤ Ã‰tat: final_results]
    
    Q --> R[ğŸ¤– NÅ’UD 5: llm_response_node]
    R --> S[ğŸª LLMHook.generate_response]
    S --> T[ğŸ“¤ Ã‰tat: llm_response]
    
    T --> U[âœ… RÃ©ponse Finale]
    
    %% Styles
    classDef nodeStyle fill:#e1f5fe,stroke:#01579b,stroke-width:2px
    classDef hookStyle fill:#fff3e0,stroke:#e65100,stroke-width:2px
    classDef stateStyle fill:#e8f5e8,stroke:#2e7d32,stroke-width:2px
    classDef parallelStyle fill:#fce4ec,stroke:#c2185b,stroke-width:2px
    
    class A,U nodeStyle
    class D,E,F,J,K,O,P,S hookStyle
    class B,G,L,M,Q,T stateStyle
    class H,I parallelStyle
```

### Ordre d'ExÃ©cution des Hooks

```mermaid
sequenceDiagram
    participant User as ğŸ‘¤ Utilisateur
    participant Graph as ğŸ“Š LangGraph
    participant HM as ğŸ›ï¸ HookManager
    participant PP as ğŸ”§ PreprocessorHook
    participant VS as ğŸ” VectorSearcherHook
    participant KS as ğŸ“ KeywordSearcherHook
    participant Post as ğŸ”„ PostprocessorHook
    participant LLM as ğŸ¤– LLMHook
    
    User->>Graph: run_hookable_hybrid_search(query)
    Graph->>HM: Initialisation avec hooks
    
    Note over Graph: ğŸ”§ NÅ’UD 1: PrÃ©processing (Ordre CRITIQUE)
    Graph->>PP: clean_query(original_query)
    PP-->>Graph: cleaned_query
    Graph->>PP: detect_intent(original_query)
    PP-->>Graph: intent
    Graph->>PP: expand_query(cleaned_query)
    PP-->>Graph: expansions
    
    Note over Graph: ğŸ”ğŸ“ NÅ’UDS 2&3: Recherches (PARALLÃˆLE)
    par Recherche Vectorielle
        Graph->>VS: search(expansions, "vector")
        VS-->>Graph: vector_results
    and Recherche Mots-clÃ©s
        Graph->>KS: search(expansions, "keyword")
        KS-->>Graph: keyword_results
    end
    
    Note over Graph: ğŸ”„ NÅ’UD 4: Post-processing (Ordre CRITIQUE)
    Graph->>Post: combine_results(vector_results, keyword_results)
    Post-->>Graph: combined_results
    Graph->>Post: rerank_results(combined_results, query, intent)
    Post-->>Graph: final_results
    
    Note over Graph: ğŸ¤– NÅ’UD 5: GÃ©nÃ©ration LLM
    Graph->>LLM: generate_response(query, context, intent)
    LLM-->>Graph: llm_response
    
    Graph-->>User: RÃ©sultat final avec rÃ©ponse LLM
```

## ğŸª SystÃ¨me de Hooks

### Interfaces de Hooks

Le systÃ¨me dÃ©finit 4 interfaces abstraites pour remplacer chaque Ã©tape :

```python
class PreprocessorHook(ABC):
    @abstractmethod
    async def clean_query(self, query: str) -> str: ...
    
    @abstractmethod
    async def expand_query(self, query: str) -> List[str]: ...
    
    @abstractmethod
    async def detect_intent(self, query: str) -> str: ...

class SearcherHook(ABC):
    @abstractmethod
    async def search(self, queries: List[str], method: str) -> List[Dict]: ...

class PostprocessorHook(ABC):
    @abstractmethod
    async def combine_results(self, vector_results, keyword_results) -> List[Dict]: ...
    
    @abstractmethod
    async def rerank_results(self, results, query, intent) -> List[Dict]: ...

class LLMHook(ABC):
    @abstractmethod
    async def generate_response(self, query: str, context: str, intent: str) -> str: ...
```

### Gestionnaire de Hooks

```python
class HookManager:
    def __init__(self):
        # ImplÃ©mentations par dÃ©faut
        self.preprocessor = DefaultPreprocessor()
        self.searcher = DefaultSearcher()
        self.postprocessor = DefaultPostprocessor()
        self.llm = DefaultLLM()
    
    def register_preprocessor(self, hook: PreprocessorHook, name: str): ...
    def register_searcher(self, hook: SearcherHook, name: str): ...
    def register_postprocessor(self, hook: PostprocessorHook, name: str): ...
    def register_llm(self, hook: LLMHook, name: str): ...
```

## âš™ï¸ Ordres de PrioritÃ© et Contraintes

### ğŸ”´ Ordre Critique (SÃ©quentiel)

**1. PrÃ©-traitement** :
```python
# OBLIGATOIRE : nettoyage AVANT expansion
cleaned = await hook.clean_query(query)        # Ã‰tape 1
expansions = await hook.expand_query(cleaned)   # Ã‰tape 2 (dÃ©pend de 1)
intent = await hook.detect_intent(query)        # Peut Ãªtre parallÃ¨le
```

**2. Post-traitement** :
```python
# OBLIGATOIRE : combinaison AVANT re-ranking
combined = await hook.combine_results(vector, keyword)  # Ã‰tape 1
reranked = await hook.rerank_results(combined)          # Ã‰tape 2 (dÃ©pend de 1)
```

### ğŸŸ¢ Ordre Flexible (ParallÃ¨le)

**Recherches hybrides** :
- `vector_search` et `keyword_search` sont **indÃ©pendants**
- Utilisent la mÃªme interface `SearcherHook.search(queries, method)`
- Peuvent s'exÃ©cuter en parallÃ¨le sans conflit

## ğŸ”§ Gestion de la Concurrence LangGraph

### Annotations pour Ã‰tats PartagÃ©s

```python
class SearchState(TypedDict):
    # Pas de conflit (un seul writer)
    original_query: str
    processed_query: str
    
    # Conflits possibles (writers multiples) - ANNOTATED
    vector_results: Annotated[List[Dict[str, Any]], add]
    keyword_results: Annotated[List[Dict[str, Any]], add]
    processing_metadata: Annotated[Dict[str, Any], merge_dicts]
    performance_metrics: Annotated[Dict[str, float], merge_dicts]
```

### Fonction de Merge PersonnalisÃ©e

```python
def merge_dicts(a: dict, b: dict) -> dict:
    """Merge deux dictionnaires de maniÃ¨re sÃ»re"""
    result = a.copy() if a else {}
    if b:
        result.update(b)
    return result
```

## ğŸ“¦ Installation

### DÃ©pendances Principales (requirements.txt)

```bash
# Core LangGraph
langgraph==0.6.4
langchain-core==0.3.27
langchain==0.3.27

# LLM Integration  
langchain-openai==0.3.28
openai==1.55.0

# Machine Learning
scikit-learn==1.5.1
numpy==2.1.0
sentence-transformers==3.0.1
torch==2.4.0

# Optional: NLP avancÃ©
spacy==3.8.2
```

### Installation

```bash
# Installer les dÃ©pendances
pip install -r requirements.txt

# Configuration OpenAI
export OPENAI_API_KEY="your-key-here"

# Lancer le script
python app.py
```

## ğŸš€ Utilisation

### Utilisation Basique (Hooks par DÃ©faut)

```python
import asyncio
from app import run_hookable_hybrid_search

async def main():
    # Utilise les implÃ©mentations par dÃ©faut
    result = await run_hookable_hybrid_search("qu'est-ce que Python ?")
    print(result["llm_response"])

asyncio.run(main())
```

### Utilisation AvancÃ©e avec Hooks PersonnalisÃ©s

```python
from app import HookManager, CustomPreprocessor, run_hookable_hybrid_search

async def main():
    # CrÃ©ation du gestionnaire
    hook_manager = HookManager()
    
    # Enregistrement d'un hook personnalisÃ©
    hook_manager.register_preprocessor(CustomPreprocessor(), "advanced")
    
    # Utilisation
    result = await run_hookable_hybrid_search(
        "qu'est-ce que Python pour le ML ?", 
        hook_manager
    )
    print(result["llm_response"])

asyncio.run(main())
```

## ğŸ”§ CrÃ©ation de Hooks PersonnalisÃ©s

### Exemple 1 : PrÃ©processeur AvancÃ©

```python
class AdvancedPreprocessor(PreprocessorHook):
    """PrÃ©processeur avec validation et synonymes Ã©tendus"""
    
    async def clean_query(self, query: str) -> str:
        # Validation de longueur
        if len(query) < 3:
            return query
        
        # Nettoyage avec regex avancÃ©
        import re
        return re.sub(r'[^\w\s\-\?]', '', query.lower().strip())
    
    async def expand_query(self, query: str) -> List[str]:
        # Expansion avec dictionnaire de synonymes
        expansions = [query]
        
        synonyms = {
            "python": ["python", "programming", "dÃ©veloppement", "langage"],
            "ml": ["machine learning", "apprentissage automatique", "IA"],
            "ai": ["intelligence artificielle", "artificial intelligence"]
        }
        
        for term, syns in synonyms.items():
            if term in query.lower():
                for syn in syns:
                    exp = query.replace(term, syn)
                    if exp not in expansions:
                        expansions.append(exp)
        
        return expansions[:5]
    
    async def detect_intent(self, query: str) -> str:
        # Patterns d'intention Ã©tendus
        patterns = {
            "definition": ["qu'est-ce", "dÃ©finir", "dÃ©finition", "c'est quoi"],
            "how_to": ["comment", "Ã©tapes", "procÃ©dure", "tutoriel"],
            "comparison": ["diffÃ©rence", "versus", "vs", "comparer"],
            "example": ["exemple", "cas d'usage", "illustration"]
        }
        
        query_lower = query.lower()
        for intent, words in patterns.items():
            if any(word in query_lower for word in words):
                return intent
        return "general"

# Enregistrement
hook_manager.register_preprocessor(AdvancedPreprocessor(), "advanced")
```

### Exemple 2 : Searcher avec Base Vectorielle

```python
class WeaviateSearcher(SearcherHook):
    """IntÃ©gration avec Weaviate"""
    
    def __init__(self, weaviate_url: str):
        import weaviate
        self.client = weaviate.Client(weaviate_url)
    
    async def search(self, queries: List[str], method: str) -> List[Dict[str, Any]]:
        if method == "vector":
            return await self._weaviate_search(queries)
        elif method == "keyword":
            return await self._weaviate_bm25_search(queries)
    
    async def _weaviate_search(self, queries: List[str]) -> List[Dict]:
        # Recherche vectorielle dans Weaviate
        results = []
        for query in queries:
            response = (
                self.client.query
                .get("Document", ["title", "content", "category"])
                .with_near_text({"concepts": [query]})
                .with_limit(5)
                .do()
            )
            # Traitement des rÃ©sultats...
        return results

# Enregistrement
hook_manager.register_searcher(WeaviateSearcher("http://localhost:8080"), "weaviate")
```

### Exemple 3 : LLM avec ModÃ¨le PersonnalisÃ©

```python
class AnthropicLLM(LLMHook):
    """Utilisation de Claude au lieu d'OpenAI"""
    
    async def generate_response(self, query: str, context: str, intent: str) -> str:
        try:
            from langchain_anthropic import ChatAnthropic
            
            llm = ChatAnthropic(
                model="claude-3-sonnet-20240229",
                temperature=0.7,
                max_tokens=500
            )
            
            prompt = f"""Tu es un assistant expert. 
            
Question: {query}
Intention: {intent}
Contexte: {context}

RÃ©ponds de maniÃ¨re structurÃ©e et prÃ©cise."""
            
            response = llm.invoke(prompt)
            return response.content
            
        except Exception as e:
            return f"Erreur Claude: {e}"

# Enregistrement
hook_manager.register_llm(AnthropicLLM(), "claude")
```

### Exemple 4 : Postprocesseur avec ML

```python
class MLPostprocessor(PostprocessorHook):
    """Re-ranking avec modÃ¨le ML"""
    
    def __init__(self):
        # Chargement d'un modÃ¨le de re-ranking
        # self.rerank_model = load_model("rerank_model.pkl")
        pass
    
    async def combine_results(self, vector_results: List[Dict], 
                            keyword_results: List[Dict]) -> List[Dict]:
        # Combinaison avec poids adaptatifs
        alpha = 0.6  # Peut Ãªtre dÃ©terminÃ© par ML
        
        # Logique de combinaison standard...
        return combined_results
    
    async def rerank_results(self, results: List[Dict], 
                           query: str, intent: str) -> List[Dict]:
        # Re-ranking avec modÃ¨le ML
        for result in results:
            # Features pour le modÃ¨le ML
            features = {
                "content_length": len(result["document"]["content"]),
                "title_query_overlap": self._calculate_overlap(
                    result["document"]["title"], query
                ),
                "intent_match": intent == result["document"].get("category", "")
            }
            
            # Score ML (simulÃ©)
            ml_score = self._predict_relevance(features)
            result["ml_score"] = ml_score
            result["combined_score"] *= (1 + ml_score)
        
        return sorted(results, key=lambda x: x["combined_score"], reverse=True)
    
    def _calculate_overlap(self, text1: str, text2: str) -> float:
        # Calcul de l'overlap entre titre et requÃªte
        words1 = set(text1.lower().split())
        words2 = set(text2.lower().split())
        return len(words1 & words2) / len(words1 | words2) if words1 | words2 else 0
    
    def _predict_relevance(self, features: Dict) -> float:
        # Simulation d'un modÃ¨le ML
        return sum(features.values()) / len(features)

# Enregistrement
hook_manager.register_postprocessor(MLPostprocessor(), "ml")
```

## ğŸ”„ Workflow avec Hooks

### Flux d'ExÃ©cution DÃ©taillÃ©

```python
# 1. PrÃ©processing avec hook
cleaned = await hook_manager.preprocessor.clean_query(query)
expansions = await hook_manager.preprocessor.expand_query(cleaned)  
intent = await hook_manager.preprocessor.detect_intent(query)

# 2. Recherches parallÃ¨les avec hook
vector_task = hook_manager.searcher.search(expansions, "vector")
keyword_task = hook_manager.searcher.search(expansions, "keyword")
vector_results, keyword_results = await asyncio.gather(vector_task, keyword_task)

# 3. Post-processing avec hook
combined = await hook_manager.postprocessor.combine_results(vector_results, keyword_results)
reranked = await hook_manager.postprocessor.rerank_results(combined, query, intent)

# 4. GÃ©nÃ©ration LLM avec hook
response = await hook_manager.llm.generate_response(query, context, intent)
```

### Configuration Multiple

```python
# Configuration pour diffÃ©rents domaines
academic_manager = HookManager()
academic_manager.register_preprocessor(AcademicPreprocessor(), "academic")
academic_manager.register_searcher(ScholarSearcher(), "scholar")

business_manager = HookManager()  
business_manager.register_preprocessor(BusinessPreprocessor(), "business")
business_manager.register_llm(BusinessLLM(), "business")

# Utilisation selon le contexte
if domain == "academic":
    result = await run_hookable_hybrid_search(query, academic_manager)
else:
    result = await run_hookable_hybrid_search(query, business_manager)
```

## ğŸ“Š MÃ©triques et ObservabilitÃ©

### MÃ©triques Automatiques

```python
{
    "performance_metrics": {
        "preprocess_time": 0.001,
        "vector_search_time": 0.018,
        "keyword_search_time": 0.002, 
        "combine_time": 0.000,
        "llm_time": 4.277,
        "total_time": 4.298
    },
    "quality_metrics": {
        "expansion_count": 3,
        "vector_results_count": 5,
        "keyword_results_count": 2,
        "final_results_count": 3
    },
    "hook_metadata": {
        "preprocessor": "advanced",
        "searcher": "default", 
        "postprocessor": "ml",
        "llm": "claude"
    }
}
```

### Logging PersonnalisÃ©

```python
class LoggingPreprocessor(PreprocessorHook):
    def __init__(self, base_preprocessor: PreprocessorHook):
        self.base = base_preprocessor
        
    async def clean_query(self, query: str) -> str:
        start = time.time()
        result = await self.base.clean_query(query)
        print(f"Clean query took {time.time() - start:.3f}s")
        return result
    
    # Wrap autres mÃ©thodes...
```

## ğŸ› RÃ©solution de ProblÃ¨mes

### Erreurs Communes

**Hook non compatible** :
```python
# âŒ Erreur
class BadHook:  # N'hÃ©rite pas de l'interface
    def search(self): pass

# âœ… Correct
class GoodHook(SearcherHook):
    async def search(self, queries: List[str], method: str) -> List[Dict]:
        return []
```

**Configuration manquante** :
```python
# âŒ Erreur : hook_manager non passÃ©
result = await run_hookable_hybrid_search(query)

# âœ… Correct
hook_manager = HookManager()
result = await run_hookable_hybrid_search(query, hook_manager)
```

**MÃ©thode async manquante** :
```python
# âŒ Erreur
def clean_query(self, query: str) -> str:  # Pas async
    return query.lower()

# âœ… Correct  
async def clean_query(self, query: str) -> str:
    return query.lower()
```

## ğŸ”§ Avantages du SystÃ¨me de Hooks

### âœ… **ModularitÃ© ComplÃ¨te**
- Chaque Ã©tape peut Ãªtre remplacÃ©e indÃ©pendamment
- Interfaces claires et testables
- Composition flexible des fonctionnalitÃ©s

### âœ… **ExtensibilitÃ©**
- Ajout de nouvelles mÃ©thodes de recherche (Weaviate, Pinecone)
- IntÃ©gration de nouveaux LLMs (Claude, Gemini)
- Algorithmes de re-ranking avancÃ©s

### âœ… **TestabilitÃ©**
- Mocks faciles pour les tests unitaires
- Isolation des composants
- Validation indÃ©pendante de chaque hook

### âœ… **Configuration Runtime**
- Changement de comportement sans redÃ©marrage
- A/B testing simple entre implÃ©mentations
- Configuration par domaine/cas d'usage

## ğŸ“ Structure du Code

```
app.py
â”œâ”€â”€ SearchState (TypedDict)              # Ã‰tat partagÃ© LangGraph
â”œâ”€â”€ Hook Interfaces                      # Abstractions
â”‚   â”œâ”€â”€ PreprocessorHook
â”‚   â”œâ”€â”€ SearcherHook  
â”‚   â”œâ”€â”€ PostprocessorHook
â”‚   â””â”€â”€ LLMHook
â”œâ”€â”€ Default Implementations              # ImplÃ©mentations par dÃ©faut
â”‚   â”œâ”€â”€ DefaultPreprocessor
â”‚   â”œâ”€â”€ DefaultSearcher
â”‚   â”œâ”€â”€ DefaultPostprocessor 
â”‚   â””â”€â”€ DefaultLLM
â”œâ”€â”€ HookManager                          # Gestionnaire centralisÃ©
â”œâ”€â”€ Graph Nodes                          # NÅ“uds utilisant les hooks
â”‚   â”œâ”€â”€ preprocess_node()
â”‚   â”œâ”€â”€ vector_search_node()
â”‚   â”œâ”€â”€ keyword_search_node()
â”‚   â”œâ”€â”€ combine_results_node()
â”‚   â””â”€â”€ llm_response_node()
â”œâ”€â”€ create_hookable_hybrid_search_graph() # Construction du graphe
â””â”€â”€ run_hookable_hybrid_search()         # Fonction principale
```

---

**ğŸ¯ Ce systÃ¨me offre une architecture complÃ¨tement modulaire oÃ¹ chaque Ã©tape peut Ãªtre remplacÃ©e par une implÃ©mentation personnalisÃ©e via des hooks, permettant une extensibilitÃ© maximale tout en conservant une interface simple d'utilisation.**
